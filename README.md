My Trasformers implementation. Using pytorch.

  Positional Encoding: Abs and RoPE.

  Attention: Multihead: Have RoPE and KV_cache (continue building, will have FlashAttn).

  LayerNorm: not yet, but i recommend the RMSNorm

  RMSNorm: done

  FFN: Base FFN use in "Attention is all u need", FFN with SiLU, MoE.
