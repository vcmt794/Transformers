My Trasformers implementation. Using pytorch.

  Positional Encoding: Abs and RoPE.

  Attention: Multihead: Have RoPE and KV_cache (continue building, will have FlashAttn).

  LayerNorm: not yet, but i recommend the RMSNorm

  RMSNorm: done

  FFN: (not yet, will have MoE).

  SwishGLU: (not yet).
