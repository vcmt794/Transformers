My Trasformers implementation. Using pytorch.

Positional Encoding: Abs and RoPE 
Attention: Multihead (building, will have KV-cached, maybe FlashAttn, too)
LayerNorm: (not yet)
FFN: (not yet, will have MoE)
SwishGLU: (not yet)
