My Trasformers implementation. Using pytorch.

  Positional Encoding: Abs and RoPE.

  Attention: Multihead: Have RoPE and KV_cache (continue building, will have FlashAttn).

  LayerNorm: (not yet).

  FFN: (not yet, will have MoE).

  SwishGLU: (not yet).
