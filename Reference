Reference:
ATTENTION IS ALL YOU NEED: https://arxiv.org/pdf/1706.03762
ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING: https://arxiv.org/pdf/2104.09864
OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER: https://arxiv.org/pdf/1701.06538
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity: https://arxiv.org/pdf/2101.03961
Layer Normalization: https://arxiv.org/pdf/1607.06450
Root Mean Square Layer Normalization:https://arxiv.org/pdf/1910.07467
